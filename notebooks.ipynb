{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesh creation in serial and parallel\n",
    "Author: JÃ¸rgen S. Dokken\n",
    "\n",
    "In this tutorial we will consider the first important class in DOLFINx, the `dolfinx.mesh.Mesh` class.\n",
    "\n",
    "A mesh consists of a set of cells. These cells can be intervals, triangles, quadrilaterals, hexahedra or tetrahedra.\n",
    "Each cell is described by a set of coordinates, and its connectivity.\n",
    "\n",
    "## Mesh creation from Numpy arrays\n",
    "For instance, let us consider a unit square. If we want to discretize it with triangular elements, we could create the set of vertices as a $(4\\times 2)$ numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tri_points = np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to decide on how we want to create the two triangles in the mesh. Let's choose the first cell to consist of vertices $0,1,3$ and the second cell consist of vertices $0,2,3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles = np.array([[0, 1, 3], [0, 2, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that for triangular cells, we could order the vertices in any order say `[[1,3,0],[2,0,3]]`, and the mesh would equivalent.\n",
    "Some finite element software reorder cells to ensure consistent integrals over interior facets.\n",
    "In DOLFINx, another strategy has been chosen, see {cite}`10.1145/3524456` for more details.\n",
    "\n",
    "Let's consider the unit square again, but this time we want to discretize it with two quadrilateral cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_points = np.array([[0.0, 0.0],[0.3, 0.0], [1.0, 0.0], [0.0, 1.0], [0.4, 1.0], [1.0, 1.0]])\n",
    "quadrilaterals = np.array([[0, 1, 3, 4], [1, 2, 4, 5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not parse the quadrilateral cell in a clockwise or counter-clockwise fashion.\n",
    "Instead, we are using a tensor product ordering.\n",
    "The ordering of the sub entities all cell types used in DOLFINx can be found at [Basix supported elements](https://github.com/FEniCS/basix/#supported-elements).\n",
    "We also note that this unit square mesh has non-affine elements.\n",
    "\n",
    "Next, we would like to generate the mesh used in DOLFINx.\n",
    "To do so, we need to generate the coordinate element.\n",
    "This is the paramterisation of each and every element, and the only way of going between the physical element and the reference element.\n",
    "We will denote any coordinate on the reference element as $\\mathbf{X}$,\n",
    "and any coordinate in the physical element as $\\mathbf{x}$,\n",
    "with the coodinate map for a given cell $M$ such that $\\mathbf{x} = M(\\mathbf{X})$.\n",
    "\n",
    "We can write \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "M(\\mathbf{X})= \\sum_{i=0}^{\\text{num vertices}} \\mathbf{v}_i\\phi_i(\\mathbf{X}\n",
    "\\end{align})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{v}_i$ is the $i$th vertex of a cell and $\\phi_i$ are the basis functions as specifed at [DefElement P1 triangles](https://defelement.com/elements/examples/triangle-Lagrange-1.html) and\n",
    "[DefElement Q1 quadrilaterals](https://defelement.com/elements/examples/quadrilateral-Q-1.html).\n",
    "\n",
    "In DOLFINx, we use [UFL (the Unified Form Language)](https://github.com/FEniCS/ufl/) to define finite elements.\n",
    "Therefore we create the `ufl.Mesh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ufl\n",
    "ufl_tri = ufl.Mesh(ufl.VectorElement(\"Lagrange\", ufl.triangle, 1))\n",
    "ufl_quad = ufl.Mesh(ufl.VectorElement(\"Lagrange\", ufl.quadrilateral, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the input we need to a DOLFINx mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dolfinx\n",
    "from mpi4py import MPI\n",
    "\n",
    "quad_mesh = dolfinx.mesh.create_mesh(\n",
    "    MPI.COMM_WORLD, quadrilaterals, quad_points, ufl_quad)\n",
    "tri_mesh = dolfinx.mesh.create_mesh(\n",
    "    MPI.COMM_WORLD, triangles, tri_points, ufl_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only input to this function we have not covered so far is `MPI.COMM_WORLD`, which is an MPI communicator.\n",
    "\n",
    "### MPI Communication\n",
    "When we run a python code with `python3 name_of_file.py`, we execute python on a single process on the computer. However, if we launch the code with `mpirun -n N python3 name_of_file.py`, we execute the code on `N` processes at the same time. The `MPI.COMM_WORLD` is the communicator among the `N` processes, which can be used to send and receive data. If we use `MPI.COMM_SELF`, the communicator will not communicate with other processes.\n",
    "When we run in serial, `MPI.COMM_WORLD` is equivalent to `MPI.COMM_SELF`.\n",
    "\n",
    "Two important values in the MPI-communicator is its `rank` and `size`.\n",
    "If we run this in serial on either of the communicators above, we get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{MPI.COMM_WORLD.rank=} {MPI.COMM_WORLD.size=}\")\n",
    "print(f\"{MPI.COMM_SELF.rank=} {MPI.COMM_SELF.size=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In jupyter noteboooks, we use [ipyparallel](https://ipyparallel.readthedocs.io/en/latest/) to start a cluster and connect to two processes, which we can execute commands on using the magic `%%px` at the top of each cell. See [%%px Cell magic](https://ipyparallel.readthedocs.io/en/latest/tutorial/magics.html#px-cell-magic) for more details.\n",
    "\n",
    "```{note}\n",
    "When starting a cluster, we do not carry ower any modules or variables from the previously executed code in the script.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

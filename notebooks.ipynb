{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesh creation in serial and parallel\n",
    "Author: JÃ¸rgen S. Dokken\n",
    "\n",
    "In this tutorial we will consider the first important class in DOLFINx, the `dolfinx.mesh.Mesh` class.\n",
    "\n",
    "A mesh consists of a set of cells. These cells can be intervals, triangles, quadrilaterals, hexahedrons or tetrahedrons.\n",
    "Each cell is described by a set of coordinates, and its connectivity.\n",
    "\n",
    "## Mesh creation from numpy arrays\n",
    "For instance, let us consider a unit square. If we want to discretize it with triangular elements, we could create the set of vertices as a $(4\\times 2)$ numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tri_points = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to decide on how we want to create the two triangles in the mesh. Let's choose the first cell to consist of vertices $0,1,3$ and the second cell consist of vertices $0,2,3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triangles = np.array([[0,1,3], [0,2,3]], dtype=np.int64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that for triangular cells, we could order the vertices in any order say `[[1,3,0],[2,0,3]]`, and the mesh would equivalent.\n",
    "Some finite element software reorder cells to ensure consistent integrals over interior facets.\n",
    "In DOLFINx, another strategy has been chosen, see {cite}`10.1145/3524456` for more details.\n",
    "\n",
    "Let's consider the unit square again, but this time we want to discretize it with two quadrilateral cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad_points = np.array([[0,0],[0.3, 0], [1, 0], [0,1], [0.4, 1], [1, 1]], dtype=np.float64)\n",
    "quadrilaterals = np.array([[0, 1, 3, 4], [1, 2, 4, 5]], dtype=np.int64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not parse the quadrilateral cell in a clockwise or counter-clockwise fashion.\n",
    "Instead, we are using a tensor product ordering.\n",
    "The ordering of the sub entities all cell types used in DOLFINx can be found at [Basix supported elements](https://github.com/FEniCS/basix/#supported-elements).\n",
    "We also note that this unit square mesh has non-affine elements.\n",
    "\n",
    "Next, we would like to generate the mesh used in DOLFINx.\n",
    "To do so, we need to generate the coordinate element.\n",
    "This is the paramterization of each an every element, and the only way of going between the physical element and the reference element.\n",
    "We will denote any coordinate on the reference element as $\\mathbf{X}$,\n",
    "and any coordinate in the physical element as $\\mathbf{x}$,\n",
    "with the mapping $M$ such that $\\mathbf{x} = M(\\mathbf{X})$.\n",
    "\n",
    "We can write \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "M(\\mathbf{X})= \\sum_{i=0}^{\\text{num vertices}} \\mathbf{v}_i\\phi_i(\\mathbf{X}\n",
    "\\end{align})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{v}_i$ is the $i$th vertex of a cell and $\\phi_i$ are the basis functions specifed at [DefElement P1 triangles](https://defelement.com/elements/examples/triangle-Lagrange-1.html) and\n",
    "[DefElement Q1 quadrilaterals](https://defelement.com/elements/examples/quadrilateral-Q-1.html).\n",
    "\n",
    "In DOLFINx we use the [Unified Form Language](https://github.com/FEniCS/ufl/) to define finite elements.\n",
    "Therefore we create the `ufl.Mesh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ufl\n",
    "ufl_tri = ufl.Mesh(ufl.VectorElement(\"Lagrange\", ufl.triangle, 1))\n",
    "ufl_quad = ufl.Mesh(ufl.VectorElement(\"Lagrange\", ufl.quadrilateral, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the input we need to a DOLFINx mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dolfinx\n",
    "from mpi4py import MPI\n",
    "\n",
    "quad_mesh = dolfinx.mesh.create_mesh(\n",
    "    MPI.COMM_WORLD, quadrilaterals, quad_points, ufl_quad)\n",
    "tri_mesh = dolfinx.mesh.create_mesh(\n",
    "    MPI.COMM_WORLD, triangles, tri_points, ufl_tri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only input to this function we have not covered so far is the `MPI.COMM_WORLD`, which is an MPI communicator.\n",
    "\n",
    "### MPI Communication\n",
    "When we run a python code with `python3 name_of_file.py`. We execute python on a single process on the computer. However, if we launch the code with `mpirun -n N python3 name_of_file.py`, we execute the code on `N` processes at the same time. The `MPI.COMM_WORLD` is the communicator among the `N` processes, which can be used to send and receive data. If we use `MPI.COMM_SELF`, the communicator will not communicate with other processes.\n",
    "When we run in serial, `MPI.COMM_WORLD` is equivalent to `MPI.COMM_SELF`.\n",
    "\n",
    "Two important values in the MPI-communicator is its `rank` and `size`.\n",
    "If we run this in serial on either of the communicators above, we get\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{MPI.COMM_WORLD.rank=} {MPI.COMM_WORLD.size=}\")\n",
    "print(f\"{MPI.COMM_SELF.rank=} {MPI.COMM_SELF.size=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In jupyter noteboooks, we use [ipyparallel](https://ipyparallel.readthedocs.io/en/latest/) to start a cluster and connect to two processes, which we can execute commands on using the magic `%%px` at the top of each cell. See [%%px Cell magic](https://ipyparallel.readthedocs.io/en/latest/tutorial/magics.html#px-cell-magic) for more details.\n",
    "\n",
    "```{note}\n",
    "When starting a cluster, we do not carry ower any modules or variables from the previously executed code in the script.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "cluster = ipp.Cluster(engines=\"mpi\", n=2)\n",
    "rc = cluster.start_and_connect_sync()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import `mpi4py` on the two engines and check the rank and size of the two processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI as MPIpx\n",
    "import numpy as np\n",
    "import ufl\n",
    "import dolfinx\n",
    "\n",
    "print(f\"{MPIpx.COMM_WORLD.rank=} {MPIpx.COMM_WORLD.size=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to create the triangle mesh, distributed over the two processes.\n",
    "We do this by sending in the points and cells for the mesh in one of two ways:\n",
    "\n",
    "**1. Send all points and cells on one process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if MPIpx.COMM_WORLD.rank == 0:\n",
    "    tri_points = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float64)\n",
    "    triangles = np.array([[0,1,3], [1,2,3]], dtype=np.int64)\n",
    "else:\n",
    "    tri_points = np.empty((0,2), dtype=np.float64)\n",
    "    triangles = np.empty((0,3), dtype=np.int64)\n",
    "ufl_tri = ufl.Mesh(ufl.VectorElement(\"Lagrange\", ufl.triangle, 1))\n",
    "tri_mesh = dolfinx.mesh.create_mesh(\n",
    "    MPIpx.COMM_WORLD, triangles, tri_points, ufl_tri)\n",
    "cell_index_map = tri_mesh.topology.index_map(tri_mesh.topology.dim)\n",
    "print(f\"Num cells local: {cell_index_map.size_local}\\n Num cells global: {cell_index_map.size_global}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output above, we see the distribution of cells on each process.\n",
    "\n",
    "**2. Distribute input of points and cells**\n",
    "\n",
    "For large meshes, reading in all points and cells on a single process would be a bottle-neck.\n",
    "Therefore, we can read in the points and cells in a distributed fashion.\n",
    "Note that if we do this it important to note that it is assumed that rank 0 has read in the first chunck of points and cells in a continuous fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "if MPIpx.COMM_WORLD.rank == 0:\n",
    "    quadrilaterals = np.array([], dtype=np.int64)\n",
    "    quad_points = np.array([[0,0],[0.3, 0]], dtype=np.float64)\n",
    "elif MPIpx.COMM_WORLD.rank == 1:\n",
    "    quadrilaterals = np.array([[0, 1, 3, 4], [1, 2, 4, 5]], dtype=np.int64)\n",
    "    quad_points = np.array([[1, 0], [0,1], [0.4, 1], [1, 1]], dtype=np.float64)\n",
    "else:\n",
    "    quad_points = np.empty((0,2), dtype=np.float64)\n",
    "    quadrilaterals = np.empty((0,4), dtype=np.int64)\n",
    "\n",
    "ufl_quad = ufl.Mesh(ufl.VectorElement(\"Lagrange\", ufl.quadrilateral, 1))\n",
    "quad_mesh = dolfinx.mesh.create_mesh(\n",
    "    MPIpx.COMM_WORLD, quadrilaterals, quad_points, ufl_quad)\n",
    "cell_index_map = quad_mesh.topology.index_map(quad_mesh.topology.dim)\n",
    "print(f\"Num cells local: {cell_index_map.size_local}\\n Num cells global: {cell_index_map.size_global}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of MPI.COMM_SELF\n",
    "You might wonder, if we can use multiple processes, when would we ever use `MPI.COMM_SELF`?\n",
    "There are many reasons for this. For instance, many simulations are too small to gain from parallelizing.\n",
    "Then one could use `MPI.COMM_SELF` with multiple processes to run parameterized studies in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "serial_points = np.array([[0,0],[0.3, 0], [1, 0], [0,1], [0.4, 1], [1, 1]], dtype=np.float64)\n",
    "serial_quads = np.array([[0, 1, 3, 4], [1, 2, 4, 5]], dtype=np.int64)\n",
    "serial_mesh = dolfinx.mesh.create_mesh(\n",
    "    MPIpx.COMM_SELF, serial_quads, serial_points, ufl_quad)\n",
    "cell_index_map = serial_mesh.topology.index_map(serial_mesh.topology.dim)\n",
    "print(f\"Num cells local: {cell_index_map.size_local}\\n Num cells global: {cell_index_map.size_global}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mesh-partitioning\n",
    "As we have seen above, we can send in data to mesh creation and get either a distributed mesh out, but how does it work?\n",
    "Under the hood, what happens is that DOLFINx calls a graph-partitioning algorithm.\n",
    "This algorithm is supplied from either from PT-Scotch{cite}`10.1016/j.parco.2007.12.001`, ParMETIS{cite}`10.1145/369028.369103` or KaHIP{cite}`10.1007/978-3-642-38527-8_16`, depending on what is available with your installation.\n",
    "\n",
    "We can list the available partitioners with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "try:\n",
    "    from dolfinx.graph import partitioner_scotch\n",
    "    has_scotch = True\n",
    "except ImportError:\n",
    "    has_scotch = False\n",
    "try:\n",
    "    from dolfinx.graph import partitioner_kahip\n",
    "    has_kahip = True\n",
    "except ImportError:\n",
    "    has_kahip = False\n",
    "try:\n",
    "    from dolfinx.graph import partitioner_parmetis\n",
    "    has_parmetis = True\n",
    "except ImportError:\n",
    "    has_parmetis = False\n",
    "print(f\"{has_scotch=}  {has_kahip=} {has_parmetis=}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given any of these partitioners (we will from now on use Scotch), you can send them into create mesh by calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "assert has_scotch\n",
    "partitioner = dolfinx.mesh.create_cell_partitioner(partitioner_scotch())\n",
    "quad_mesh = dolfinx.mesh.create_mesh(\n",
    "    MPIpx.COMM_WORLD, quadrilaterals, quad_points, ufl_quad, partitioner=partitioner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
